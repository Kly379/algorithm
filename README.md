一、ReLU经典变体
1. Leaky ReLU（带泄漏的ReLU）
公式

$$LeakyReLU(x) = \begin{cases}x, & x\geq0 \\ \alpha x, & x<0\end{cases}$$

$\alpha$ 为小正数（默认0.01，可手动调整，非可学习参数）

核心改进

x<0 时保留微小梯度（$\alpha$），避免神经元永久死亡。

优缺点

-  优点：解决死亡ReLU问题，计算效率与ReLU接近，无需额外参数量

-  缺点：$\alpha$ 为超参数，需人工调优，负区间梯度固定，适应性一般

适用场景

替换ReLU解决神经元死亡的场景，适配CNN、MLP隐藏层。

2. PReLU（参数化ReLU）
公式

$$PReLU(x) = \begin{cases}x, & x\geq0 \\ \alpha_i x, & x<0\end{cases}$$

核心改进

将LeakyReLU的固定$\alpha$改为可学习参数（$\alpha_i$对应第$i$个通道/神经元），由网络训练自动优化。

优缺点

-  优点：自适应调整负区间梯度，拟合能力更强

-  缺点：增加少量参数量，小数据集下易过拟合

适用场景

大数据集的CNN模型（如ResNet），不推荐小样本场景。

3. RReLU（随机化ReLU）
公式

$$RReLU(x) = \begin{cases}x, & x\geq0 \\ \alpha x, & x<0\end{cases}$$

训练时$\alpha$随机取$[l, u]$内均匀分布（如$l=0.1, u=0.3$），测试时固定为均值。

核心改进

通过随机化引入正则化效果，防止过拟合，同时解决死亡ReLU问题。

优缺点

-  优点：兼顾抗死亡ReLU和正则化，无额外参数量

-  缺点：训练过程存在随机性，模型复现性略低

适用场景

小数据集的分类/回归任务，CNN、MLP均适配。

4. ELU（指数线性单元）
公式

$$ELU(x) = \begin{cases}x, & x\geq0 \\ \alpha (e^x - 1), & x<0\end{cases}$$

$\alpha$ 通常取1，可手动调整。

核心改进

- x<0时梯度为$\alpha e^x$，平滑无硬阈值，缓解死亡ReLU

- 输出零中心化（均值接近0），优化后续层梯度传播

优缺点

-  优点：零中心输出，梯度更稳定，抗死亡ReLU效果优于Leaky ReLU

-  缺点：引入指数运算，计算效率低于ReLU、Leaky ReLU

适用场景

对梯度稳定性要求高的模型，不推荐对推理速度敏感的场景（如端侧设备）。

5. SELU（缩放指数线性单元）
公式

$$SELU(x) = \lambda \begin{cases}x, & x\geq0 \\ \alpha (e^x - 1), & x<0\end{cases}$$

$\lambda\approx1.0507$，$\alpha\approx1.6732$（固定常数，由理论推导得到，无需调优）。

核心改进

基于ELU增加固定缩放因子$\lambda$，实现自归一化（Self-Normalization），网络各层输出均值和方差自动稳定，无需BatchNorm。

优缺点

-  优点：自归一化，可省略BatchNorm，深层网络梯度更稳定

-  缺点：计算成本高，仅适配特定网络结构（需用He/LeCun初始化）

适用场景

深层MLP、CNN（如深度超过50层的网络），不适用于Transformer。


二、其他激活函数
1. Sigmoid（逻辑斯蒂函数）
公式

$$Sigmoid(x) = \frac{1}{1+e^{-x}}$$

核心特性

- 输出映射到 $[0,1]$，可直接表示概率（如正类概率）

- 零阶可导，非线性平滑

优缺点

-  优点：输出可解释为概率，适合二分类场景

-  缺点：梯度消失严重（$|x|>3$ 时梯度趋近于0）、输出非零中心、计算含指数运算

适用场景

二分类任务的输出层（表示正类概率），几乎不用于深层网络隐藏层。

2. Tanh（双曲正切函数）
公式

$$Tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

核心特性

- 输出映射到 $[-1,1]$，零中心化（均值接近0）

- 梯度消失问题比Sigmoid轻微，但仍存在

优缺点

-  优点：零中心输出，梯度消失比Sigmoid轻

-  缺点：$|x|>3$ 时仍会梯度消失，计算成本高

适用场景

传统模型（如RNN）的隐藏层，或需要输出正负值的场景，现已逐步被ReLU变体替代。

3. Softmax
公式

$$Softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}$$（$i=1,2,...,n$）

核心特性

- 将$n$维输入映射到 $[0,1]$，且所有输出和为1，可表示多分类的概率分布

- 对输入尺度敏感，易受异常值影响

优缺点

-  优点：可直接输出多分类概率分布，适配多分类任务

-  缺点：对输入尺度敏感，计算过程中可能出现数值溢出（工程中常做数值稳定处理）

适用场景

多分类任务的输出层（如图像分类、文本分类），不可用于隐藏层。
